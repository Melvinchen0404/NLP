{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Tokenization.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyOsqFUCASmFUhbQlx6sYLh+",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/gist/Melvinchen0404/8ed2215da4ad813d8e24858324b016f5/tokenization.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##NLP Technique 1: Tokenization\n",
        "**Tokenization** refers to the process of breaking a piece of raw text into smaller units (called **tokens**) for processing \\\n",
        "\\\n",
        "**STEP 1:** Download the **Natural Language Toolkit (NLTK)**: https://www.nltk.org/ The **Natural Language Toolkit (NLTK)** is an open source Python library for Natural Language Processing \\\n",
        "**STEP 2:** Import the `nltk` package "
      ],
      "metadata": {
        "id": "KFSt_wU7hacg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "nltk.download('punkt')\n",
        "\n",
        "class color:\n",
        "   BOLD = '\\033[1m'\n",
        "   END = '\\033[0m'"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yXYwCLOJqAdX",
        "outputId": "c9e7f30e-ef44-442a-bd19-61ace84e8553"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**STEP 3:** Import the relevant tokenization functions (`word_tokenize`, `sent_tokenize`) \\\n",
        "**STEP 4:** For our example, we are using the first two lines from James Joyce's *Ulysses* as our text (https://www.gutenberg.org/files/4300/4300-h/4300-h.htm#chap01) \\\n",
        "**STEP 5:** Get rid of punctuation in **word tokenization** by using the `.isalnum()` function: this function checks whether a character in a string is alphanumeric or not and only retains the alphanumeric characters"
      ],
      "metadata": {
        "id": "WnenYw4iqC9F"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "z6uembFpg0W8",
        "outputId": "d3c77494-a0e5-4cc2-e78a-beb89d08f563"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1mOriginal text: \n",
            "\u001b[0m Stately, plump Buck Mulligan came from the stairhead, bearing a bowl of lather on which a mirror and a razor lay crossed. A yellow dressinggown, ungirdled, was sustained gently behind him on the mild morning air.\n",
            "\u001b[1mTokenized words: \n",
            "\u001b[0m ['Stately', 'plump', 'Buck', 'Mulligan', 'came', 'from', 'the', 'stairhead', 'bearing', 'a', 'bowl', 'of', 'lather', 'on', 'which', 'a', 'mirror', 'and', 'a', 'razor', 'lay', 'crossed', 'A', 'yellow', 'dressinggown', 'ungirdled', 'was', 'sustained', 'gently', 'behind', 'him', 'on', 'the', 'mild', 'morning', 'air']\n"
          ]
        }
      ],
      "source": [
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.tokenize import sent_tokenize\n",
        "from nltk.probability import FreqDist\n",
        "text = \"Stately, plump Buck Mulligan came from the stairhead, bearing a bowl of lather on which a mirror and a razor lay crossed. A yellow dressinggown, ungirdled, was sustained gently behind him on the mild morning air.\"\n",
        "tokens1 = nltk.word_tokenize(text)\n",
        "tokens2 = [word for word in tokens1 if word.isalnum()]\n",
        "print(color.BOLD + 'Original text: \\n' + color.END, text)\n",
        "print(color.BOLD + 'Tokenized words: \\n' + color.END, tokens2)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**STEP 6:** The `FreqDist` function will allow us to count the number of words in a string \\\n",
        "**STEP 7**: When we apply `most_common()`, we will return a list of the *n* most common elements and their counts from the most common to the least. This will yield the frequency of each word, from the most common to the least common"
      ],
      "metadata": {
        "id": "sjZq_y0Io_O2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "fdist1 = FreqDist(tokens2)\n",
        "print(color.BOLD + 'Number of distinct and tokenized words (samples) and number of tokenized words (outcomes): \\n' + color.END, fdist1)\n",
        "print(color.BOLD + 'Frequency of each distinct and tokenized word: \\n' + color.END, fdist1.most_common())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6EdXSBYfpt1c",
        "outputId": "1085f31c-8382-4cb6-8355-feb84766fb0e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1mNumber of distinct and tokenized words (samples) and number of tokenized words (outcomes): \n",
            "\u001b[0m <FreqDist with 32 samples and 36 outcomes>\n",
            "\u001b[1mFrequency of each distinct and tokenized word: \n",
            "\u001b[0m [('a', 3), ('the', 2), ('on', 2), ('Stately', 1), ('plump', 1), ('Buck', 1), ('Mulligan', 1), ('came', 1), ('from', 1), ('stairhead', 1), ('bearing', 1), ('bowl', 1), ('of', 1), ('lather', 1), ('which', 1), ('mirror', 1), ('and', 1), ('razor', 1), ('lay', 1), ('crossed', 1), ('A', 1), ('yellow', 1), ('dressinggown', 1), ('ungirdled', 1), ('was', 1), ('sustained', 1), ('gently', 1), ('behind', 1), ('him', 1), ('mild', 1), ('morning', 1), ('air', 1)]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**STEP 8**: We can tokenize the sentences in the text too"
      ],
      "metadata": {
        "id": "hnSUqYXBrWP7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "fdist2 = FreqDist(sent_tokenize(text))\n",
        "print(color.BOLD + 'Tokenized sentences: \\n' + color.END, sent_tokenize(text))\n",
        "print(color.BOLD + 'Number of distinct and tokenized sentences (samples) and number of tokenized sentences (outcomes): \\n' + color.END, fdist2)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "x_Vdc-ionD8v",
        "outputId": "aca95718-cd03-4659-e3b0-1511b6343645"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1mTokenized sentences: \n",
            "\u001b[0m ['Stately, plump Buck Mulligan came from the stairhead, bearing a bowl of lather on which a mirror and a razor lay crossed.', 'A yellow dressinggown, ungirdled, was sustained gently behind him on the mild morning air.']\n",
            "\u001b[1mNumber of distinct and tokenized sentences (samples) and number of tokenized sentences (outcomes): \n",
            "\u001b[0m <FreqDist with 2 samples and 2 outcomes>\n"
          ]
        }
      ]
    }
  ]
}